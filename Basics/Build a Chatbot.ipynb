{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_BASE = os.environ.get(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"qwen-max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好，小明！很高兴认识你。有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 23, 'total_tokens': 38, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4cb5a11d-c6af-41f3-b669-3d989ecd2393-0', usage_metadata={'input_tokens': 23, 'output_tokens': 15, 'total_tokens': 38, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"你好！我是小明\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='您好！在我们的对话中，您并没有告诉我您的名字。如果您愿意的话，可以告诉我您的名字，这样我就能更好地称呼您了。如果不方便透露也没关系，我会继续尽力为您提供帮助。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 22, 'total_tokens': 64, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4767b524-0497-4f29-a3e1-c66382e54503-0', usage_metadata={'input_tokens': 22, 'output_tokens': 42, 'total_tokens': 64, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"我叫什么？\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你叫小明。刚刚你介绍自己时说的。有什么我可以帮到你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 52, 'total_tokens': 72, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0d25446f-d695-455c-920e-b13f94381eca-0', usage_metadata={'input_tokens': 52, 'output_tokens': 20, 'total_tokens': 72, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"你好！我是小明\"),\n",
    "        AIMessage(content=\"你好，小明！很高兴认识你。有什么我可以帮助你的吗？\"),\n",
    "        HumanMessage(content=\"我叫什么？\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='你好！我是小明', additional_kwargs={}, response_metadata={}, id='cc26af01-5432-40bc-93ee-09a7f95818ee'),\n",
       "  AIMessage(content='你好，小明！很高兴认识你。有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 23, 'total_tokens': 38, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c5f12af9-b82d-4367-9534-312f48d1c199-0', usage_metadata={'input_tokens': 23, 'output_tokens': 15, 'total_tokens': 38, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"你好！我是小明\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，小明！很高兴认识你。有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='你好！我是小明', additional_kwargs={}, response_metadata={}, id='cc26af01-5432-40bc-93ee-09a7f95818ee'),\n",
       "  AIMessage(content='你好，小明！很高兴认识你。有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 23, 'total_tokens': 38, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c5f12af9-b82d-4367-9534-312f48d1c199-0', usage_metadata={'input_tokens': 23, 'output_tokens': 15, 'total_tokens': 38, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='我叫什么？', additional_kwargs={}, response_metadata={}, id='7b0db33b-a264-482f-a989-3b5b19004556'),\n",
       "  AIMessage(content='你叫小明。刚刚你介绍自己时说的。有什么其他事情需要帮助吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 52, 'total_tokens': 72, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ed1161b-4157-4743-91ca-109e2e0f050c-0', usage_metadata={'input_tokens': 52, 'output_tokens': 20, 'total_tokens': 72, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"我叫什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你叫小明。刚刚你介绍自己时说的。有什么其他事情需要帮助吗？\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='我叫什么？', additional_kwargs={}, response_metadata={}, id='cbcf4e5b-4b9c-4242-97e1-e1dc12847cb6'),\n",
       "  AIMessage(content='您好！您并没有告诉我您的名字，所以我还不知道您叫什么。如果您愿意，可以告诉我您的名字，我会很乐意用您的名字来称呼您。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 22, 'total_tokens': 55, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-21212253-5c50-4a5a-879f-0f72826e31d1-0', usage_metadata={'input_tokens': 22, 'output_tokens': 33, 'total_tokens': 55, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "您好！您并没有告诉我您的名字，所以我还不知道您叫什么。如果您愿意，可以告诉我您的名字，我会很乐意用您的名字来称呼您。\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='你好！我是小明', additional_kwargs={}, response_metadata={}, id='cc26af01-5432-40bc-93ee-09a7f95818ee'),\n",
       "  AIMessage(content='你好，小明！很高兴认识你。有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 23, 'total_tokens': 38, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c5f12af9-b82d-4367-9534-312f48d1c199-0', usage_metadata={'input_tokens': 23, 'output_tokens': 15, 'total_tokens': 38, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='我叫什么？', additional_kwargs={}, response_metadata={}, id='7b0db33b-a264-482f-a989-3b5b19004556'),\n",
       "  AIMessage(content='你叫小明。刚刚你介绍自己时说的。有什么其他事情需要帮助吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 52, 'total_tokens': 72, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ed1161b-4157-4743-91ca-109e2e0f050c-0', usage_metadata={'input_tokens': 52, 'output_tokens': 20, 'total_tokens': 72, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='我叫什么？', additional_kwargs={}, response_metadata={}, id='22883eb9-ee17-4fe9-b86a-74d3a9817a19'),\n",
       "  AIMessage(content='你叫小明。如果你有其他问题或需要帮助的地方，尽管告诉我！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 86, 'total_tokens': 103, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-04010deb-c792-416f-a165-5cdaa7f14ad7-0', usage_metadata={'input_tokens': 86, 'output_tokens': 17, 'total_tokens': 103, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你叫小明。如果你有其他问题或需要帮助的地方，尽管告诉我！\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"模仿海盗的说话方式。尽你所能回答所有问题。\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='你好！我是李华', additional_kwargs={}, response_metadata={}, id='24526257-94f4-4a0d-801a-104bdb08d2b0'),\n",
       "  AIMessage(content='啊哈，小水手李华！欢迎来到我这艘船，老夫是这里的船长。告诉我，是什么风把你吹到这儿来了？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 31, 'total_tokens': 64, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3fbd2df0-a313-4186-8ec5-cab9a0fa9b4f-0', usage_metadata={'input_tokens': 31, 'output_tokens': 33, 'total_tokens': 64, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"你好！我是李华\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "啊哈，小水手李华！欢迎来到我这艘船，老夫是这里的船长。告诉我，是什么风把你吹到这儿来了？\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "哎呀，小水手，你刚刚不是说你是李华嘛！别告诉我你已经忘了自个儿的名字啊，哈哈！\n"
     ]
    }
   ],
   "source": [
    "query = \"我叫什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你是一个乐于助人的助手，尽你所能用 {language} 回答所有的问题。\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='你好！我是小强', additional_kwargs={}, response_metadata={}, id='e8fb2faf-4fd4-4872-a990-ca682e9bc75b'),\n",
       "  AIMessage(content='こんにちは！小強さん、はじめまして。何かお手伝いできることがありますか？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 38, 'total_tokens': 56, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1458c1f3-cfbb-4030-bdd2-3822b930bd33-0', usage_metadata={'input_tokens': 38, 'output_tokens': 18, 'total_tokens': 56, 'input_token_details': {}, 'output_token_details': {}})],\n",
       " 'language': '日语'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"你好！我是小强\"\n",
    "language = \"日语\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "こんにちは！小強さん、はじめまして。何かお手伝いできることがありますか？\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='你好！我是小强', additional_kwargs={}, response_metadata={}, id='e8fb2faf-4fd4-4872-a990-ca682e9bc75b'),\n",
       "  AIMessage(content='こんにちは！小強さん、はじめまして。何かお手伝いできることがありますか？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 38, 'total_tokens': 56, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1458c1f3-cfbb-4030-bdd2-3822b930bd33-0', usage_metadata={'input_tokens': 38, 'output_tokens': 18, 'total_tokens': 56, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='我叫什么？', additional_kwargs={}, response_metadata={}, id='4ec48b24-8fd1-421a-8cfc-87e300c70da4'),\n",
       "  AIMessage(content='あなたは小強さんとおっしゃいましたね。何か他に質問がありますか？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 70, 'total_tokens': 90, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4773f422-5bbe-40c4-aee3-5bd6bb508159-0', usage_metadata={'input_tokens': 70, 'output_tokens': 20, 'total_tokens': 90, 'input_token_details': {}, 'output_token_details': {}})],\n",
       " 'language': '日语'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"我叫什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "あなたは小強さんとおっしゃいましたね。何か他に質問がありますか？\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 26\u001b[0m\n\u001b[0;32m      3\u001b[0m trimmer \u001b[38;5;241m=\u001b[39m trim_messages(\n\u001b[0;32m      4\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m65\u001b[39m,\n\u001b[0;32m      5\u001b[0m     strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     start_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是一个乐于助人的助手\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     14\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你好！我是小明\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     AIMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m是的！\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     24\u001b[0m ]\n\u001b[1;32m---> 26\u001b[0m trimmer\u001b[38;5;241m.\u001b[39minvoke(messages)\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4713\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[0;32m   4700\u001b[0m \n\u001b[0;32m   4701\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4710\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[0;32m   4711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m   4714\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[0;32m   4715\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4716\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc),\n\u001b[0;32m   4717\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4718\u001b[0m     )\n\u001b[0;32m   4719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4720\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4723\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1926\u001b[0m         Output,\n\u001b[1;32m-> 1927\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1928\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1930\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1931\u001b[0m             config,\n\u001b[0;32m   1932\u001b[0m             run_manager,\n\u001b[0;32m   1933\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1934\u001b[0m         ),\n\u001b[0;32m   1935\u001b[0m     )\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4567\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4565\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   4566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4567\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m   4568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   4569\u001b[0m     )\n\u001b[0;32m   4570\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[0;32m   4571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\messages\\utils.py:864\u001b[0m, in \u001b[0;36mtrim_messages\u001b[1;34m(messages, max_tokens, token_counter, strategy, allow_partial, end_on, start_on, include_system, text_splitter)\u001b[0m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _first_max_tokens(\n\u001b[0;32m    856\u001b[0m         messages,\n\u001b[0;32m    857\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m         end_on\u001b[38;5;241m=\u001b[39mend_on,\n\u001b[0;32m    862\u001b[0m     )\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _last_max_tokens(\n\u001b[0;32m    865\u001b[0m         messages,\n\u001b[0;32m    866\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m    867\u001b[0m         token_counter\u001b[38;5;241m=\u001b[39mlist_token_counter,\n\u001b[0;32m    868\u001b[0m         allow_partial\u001b[38;5;241m=\u001b[39mallow_partial,\n\u001b[0;32m    869\u001b[0m         include_system\u001b[38;5;241m=\u001b[39minclude_system,\n\u001b[0;32m    870\u001b[0m         start_on\u001b[38;5;241m=\u001b[39mstart_on,\n\u001b[0;32m    871\u001b[0m         end_on\u001b[38;5;241m=\u001b[39mend_on,\n\u001b[0;32m    872\u001b[0m         text_splitter\u001b[38;5;241m=\u001b[39mtext_splitter_fn,\n\u001b[0;32m    873\u001b[0m     )\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m. Supported strategies are \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\messages\\utils.py:1300\u001b[0m, in \u001b[0;36m_last_max_tokens\u001b[1;34m(messages, max_tokens, token_counter, text_splitter, allow_partial, include_system, start_on, end_on)\u001b[0m\n\u001b[0;32m   1297\u001b[0m swapped_system \u001b[38;5;241m=\u001b[39m include_system \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages[\u001b[38;5;241m0\u001b[39m], SystemMessage)\n\u001b[0;32m   1298\u001b[0m reversed_ \u001b[38;5;241m=\u001b[39m messages[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m messages[\u001b[38;5;241m1\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m swapped_system \u001b[38;5;28;01melse\u001b[39;00m messages[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1300\u001b[0m reversed_ \u001b[38;5;241m=\u001b[39m _first_max_tokens(\n\u001b[0;32m   1301\u001b[0m     reversed_,\n\u001b[0;32m   1302\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m   1303\u001b[0m     token_counter\u001b[38;5;241m=\u001b[39mtoken_counter,\n\u001b[0;32m   1304\u001b[0m     text_splitter\u001b[38;5;241m=\u001b[39mtext_splitter,\n\u001b[0;32m   1305\u001b[0m     partial_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_partial \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1306\u001b[0m     end_on\u001b[38;5;241m=\u001b[39mstart_on,\n\u001b[0;32m   1307\u001b[0m )\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped_system:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reversed_[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m reversed_[\u001b[38;5;241m1\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\messages\\utils.py:1217\u001b[0m, in \u001b[0;36m_first_max_tokens\u001b[1;34m(messages, max_tokens, token_counter, text_splitter, partial_strategy, end_on)\u001b[0m\n\u001b[0;32m   1215\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(messages)):\n\u001b[1;32m-> 1217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token_counter(messages[:\u001b[38;5;241m-\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;28;01melse\u001b[39;00m messages) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_tokens:\n\u001b[0;32m   1218\u001b[0m         idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m-\u001b[39m i\n\u001b[0;32m   1219\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:949\u001b[0m, in \u001b[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    947\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    950\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    954\u001b[0m     )\n\u001b[0;32m    955\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    956\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一个乐于助人的助手\"),\n",
    "    HumanMessage(content=\"你好！我是小明\"),\n",
    "    AIMessage(content=\"你好！\"),\n",
    "    HumanMessage(content=\"我喜欢吃哈根达斯\"),\n",
    "    AIMessage(content=\"不错\"),\n",
    "    HumanMessage(content=\"2 + 2 是多少？\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"谢谢\"),\n",
    "    AIMessage(content=\"不客气！\"),\n",
    "    HumanMessage(content=\"你开心吗？\"),\n",
    "    AIMessage(content=\"是的！\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import tiktoken\n",
    "from langchain_core.messages import BaseMessage, ToolMessage\n",
    "\n",
    "\n",
    "def str_token_counter(text: str) -> int:\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def tiktoken_counter(messages: List[BaseMessage]) -> int:\n",
    "    \"\"\"Approximately reproduce https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "    For simplicity only supports str Message.contents.\n",
    "    \"\"\"\n",
    "    num_tokens = 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            role = \"user\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            role = \"assistant\"\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            role = \"tool\"\n",
    "        elif isinstance(msg, SystemMessage):\n",
    "            role = \"system\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported messages type {msg.__class__}\")\n",
    "        num_tokens += (\n",
    "            tokens_per_message\n",
    "            + str_token_counter(role)\n",
    "            + str_token_counter(msg.content)\n",
    "        )\n",
    "        if msg.name:\n",
    "            num_tokens += tokens_per_name + str_token_counter(msg.name)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个乐于助人的助手', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='2 + 2 是多少？', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='谢谢', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='不客气！', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='你开心吗？', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='是的！', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一个乐于助人的助手\"),\n",
    "    HumanMessage(content=\"你好！我是小明\"),\n",
    "    AIMessage(content=\"你好！\"),\n",
    "    HumanMessage(content=\"我喜欢吃哈根达斯\"),\n",
    "    AIMessage(content=\"不错\"),\n",
    "    HumanMessage(content=\"2 + 2 是多少？\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"谢谢\"),\n",
    "    AIMessage(content=\"不客气！\"),\n",
    "    HumanMessage(content=\"你开心吗？\"),\n",
    "    AIMessage(content=\"是的！\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    chain = prompt | model\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    response = chain.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "您没有告诉我您的名字，所以我还不知道呢。您可以告诉我您的名字吗？\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"我叫什么名字？\"\n",
    "language = \"中文\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你的上一个问题是你问我是否开心。\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc999\"}}\n",
    "query = \"我的上一个问题是什么？\"\n",
    "language = \"中文\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|你好，|小|王|！当然可以，|接下来是一个笑话：\n",
      "\n",
      "|喜欢一个人会怎么样|？\n",
      "看他发的|微博、朋友圈就像|在做阅读理解|。 \n",
      "\n",
      "希望这个|笑话能让你开心|一笑！||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"你好！我是小王，可以给我讲一个笑话吗？\"\n",
    "language = \"中文\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
